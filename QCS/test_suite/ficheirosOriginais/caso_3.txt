Mike Mayberry: We’re trying to figure out what the next wave of AI is. The original wave of AI is based on logic and it’s based on writing down rules; it’s closest to what you’d call classical reasoning. The current wave of AI is around sensing and perception—using a convolutional neural net to scan an image and see if something of interest is there. Those two by themselves don’t add up to all the things that human beings do naturally as they navigate the world.

An example of this would be where you are startled by something—let’s say a car siren. You’d automatically be thinking of different scenarios that would be consistent with the data you have and you would also be conscious of the data you don’t have. You would be inferring a probability. Maybe the probability is figuring out whether the siren is coming from ahead of you or behind you. Or whether it is going to make you late for a meeting. You automatically do things that machines have trouble with. We run into those situations all the time in real life, because there’s always uncertainty around what is the current situation.

Currently AI and deep learning systems have been described as brittle. What we mean by that is they are overconfident in their answer. They’ll say with 99 percent certainty that there something in a picture that it thinks it recognizes. But in many cases the probability is incorrect; confidence is not as high as [the AI] thinks it is.

So what we’d like to do in a general research thrust is figure out how to build probability into our reasoning systems and into our sensing systems. And there’s really two challenges in that. One is the problem of how you compute with probabilities and the other is how do you store memories or scenarios with probabilities.

So we’ve been doing a certain amount of internal work and with academia, and we’ve decided that there’s enough here that we’re going to kick off a research community. The goal is to have people share what they know about it, collaborate on it, figure out how you represent probability when you write software, and how you construct computer hardware. We think this will be ... part of the third wave of AI. We don’t think we’re done there, we think there are other things as well, but this will be around probabilistic computing.

Spectrum: That term has been used to describe many things in the past that aren’t related to AI, such as stochastic computing and error-tolerant computing. What’s it really like?

Mayberry: We’re using [probabilistic computing] in a slightly different sense than before. For example, stochastic computing is about getting a good enough answer even with errors. Fuzzy logic is actually closer to the concept that we’re talking here, where you’re deliberately keeping track of uncertainties as you process information. There’s statistical computing too, which is really more of a software approach, where you’re keeping track of probabilities by building trees. So again, these are not necessarily new concepts. But we intend to apply them differently than has been done in the past.

Spectrum: Will this involve new kinds of devices?

Mayberry: We’re going to approach it initially by looking at algorithms. Our bias at Intel is to build hardware, but if we don’t really understand how the use model is going to evolve or how the algorithms are going to evolve then we run the risk of commiting to a path too early. So we’re initially going to have research thrusts around algorithms and software frameworks. There will be a piece that will be around what would hardware optimization look like if you got to that point. And, can these things be fooled? You have to think about security early on. Those are the things we’ll be approaching.
